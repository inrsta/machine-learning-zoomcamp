{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "usedcars_df = pd.read_parquet(r'data/transformed_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train (60%) and temporary set (40%)\n",
    "train_df, temp_df = train_test_split(usedcars_df, test_size=0.4, random_state=1)\n",
    "\n",
    "# Split the temporary set into validation (50%) and test (50%) sets\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y_train = train_df.price_in_euro.values\n",
    "y_valid = valid_df.price_in_euro.values\n",
    "y_test = test_df.price_in_euro.values\n",
    "\n",
    "full_train_df = train_df.copy()\n",
    "full_valid_df = valid_df.copy()\n",
    "full_test_df = test_df.copy()\n",
    "\n",
    "del train_df['price_in_euro']\n",
    "del valid_df['price_in_euro']\n",
    "del test_df['price_in_euro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"brand\", \"model\", \"color\", \"registration_date\", \"year\", \"transmission_type\", \"fuel_type\"]\n",
    "numerical_columns = [\"power_kw\", \"power_ps\", \"fuel_consumption_l_100km\", \"mileage_in_km\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132719, 11) (44240, 11) (44240, 11)\n"
     ]
    }
   ],
   "source": [
    "print(train_df[categorical_columns + numerical_columns].shape, valid_df[categorical_columns + numerical_columns].shape, test_df[categorical_columns + numerical_columns].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "\n",
    "train_dict = train_df[categorical_columns + numerical_columns].to_dict(orient='records')\n",
    "\n",
    "valid_dict = valid_df[categorical_columns + numerical_columns].to_dict(orient='records')\n",
    "\n",
    "test_dict = test_df[categorical_columns + numerical_columns].to_dict(orient='records')\n",
    "\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "X_valid = dv.transform(valid_dict)\n",
    "X_test = dv.transform(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "\n",
    "# model.fit(X_train[:50000], y_train[:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# # Assuming your data is already prepared and split into X_train, X_valid, y_train, y_valid\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'colsample_bytree': [0.3, 0.5, 0.7],\n",
    "#     'learning_rate': [0.05, 0.1, 0.15],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'alpha': [5, 10, 15],\n",
    "#     'n_estimators': [10]\n",
    "# }\n",
    "\n",
    "# # Initialize the XGBoost Regressor\n",
    "# xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# # Set up GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "#                            scoring='neg_mean_squared_error', cv=3, verbose=1)\n",
    "\n",
    "# # Fit the model\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best performing model\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# # Predictions on the validation set\n",
    "# y_pred = best_model.predict(X_valid)\n",
    "\n",
    "# # Calculate percentage differences\n",
    "# percentage_differences = [(abs(a - p) / a) * 100 if a != 0 else 0 for a, p in zip(y_valid, y_pred)]\n",
    "\n",
    "# # Compute average precision\n",
    "# average_precision = sum(percentage_differences) / len(percentage_differences)\n",
    "# print(f\"Average Precision: {average_precision:.2f}%\")\n",
    "\n",
    "# # You can also retrieve the best parameters like this\n",
    "# print(\"Best parameters found: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 73.41%\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', \n",
    "                         colsample_bytree = 0.3, \n",
    "                         learning_rate = 0.1,\n",
    "                         max_depth = 5, \n",
    "                         alpha = 10, \n",
    "                         n_estimators = 10)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_valid)\n",
    "percentage_differences = []\n",
    "\n",
    "for actual, predicted in zip(y_valid, y_pred):\n",
    "    if actual != 0:  # Avoid division by zero\n",
    "        difference = abs(actual - predicted) / actual\n",
    "        percentage_differences.append(difference * 100)\n",
    "    else:\n",
    "        # Handle the case where the actual value is 0 if needed\n",
    "        percentage_differences.append(0)  # or some other logic\n",
    "\n",
    "average_precision = sum(percentage_differences) / len(percentage_differences)\n",
    "print(f\"Average Precision: {average_precision:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "with open(r'model/best_xgboost_model.pkl', 'wb') as file:\n",
    "    pickle.dump(xgb_model, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlzoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
